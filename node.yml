---
 - hosts: other_node
   gather_facts: yes
   remote_user: panchu
   become: yes
   roles:
     - hosts
     - java
     - scala
     - node











#   tasks:
#     - name: copying the file
#       copy:
#          src: /usr/local/spark/spark-2.3.0-bin-hadoop2.7/conf/spark-env.sh.template
#          dest: /usr/local/spark/spark-2.3.0-bin-hadoop2.7/conf/spark-env.sh


#     - name: Editing the file
#       lineinfile:
#            path: /usr/local/spark/spark-2.3.0-bin-hadoop2.7/conf/spark-env.sh
#            line: export SPARK_MASTER_HOST='10.1.53.151'
#            line: export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk.x86_64

#     - name: Add the port
#       lineinfile:
#            path: /usr/local/spark/spark-2.3.0-bin-hadoop2.7/conf/spark-defaults.conf.template
#            line: spark.master spark://Master:7077


#     - name: Copying the slave file
#       copy:
#          src: /usr/local/spark/spark-2.3.0-bin-hadoop2.7/conf/slaves.template
#          dest: /usr/local/spark/spark-2.3.0-bin-hadoop2.7/conf/slaves

#     - name: Remove localhost
#       lineinfile:
#            dest: /usr/local/spark/spark-2.3.0-bin-hadoop2.7/conf/slaves
#            regexp: "localhost"
#            state: absent


#     - name: Edit the slaves file
#       blockinfile:
#           path: /usr/local/spark/spark-2.3.0-bin-hadoop2.7/conf/slaves
#           block: |
#              Master
#              Node


#     - name: Start the master
#       shell: start-master.sh
#       args:
#          chdir: /usr/local/spark/spark-2.3.0-bin-hadoop2.7/sbin

#     - name: Start the slave
#       shell: start-slaves.sh
#       args:
#          chdir: /usr/local/spark/spark-2.3.0-bin-hadoop2.7/sbin


#     - name: To check deamons on master and slaves started
#       command: cd /usr/local/spark/spark-2.3.0-bin-hadoop2.7
#       command: jps


